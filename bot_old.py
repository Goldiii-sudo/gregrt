import asyncio
import base64
import json
import logging
import uuid
import zipfile
from io import BytesIO
from os import getenv
from pathlib import Path

import aiohttp
import httpx
import requests
from aiogram import Bot, Dispatcher, Router, F
from aiogram.filters import CommandStart, Command
from aiogram.types import Message, BufferedInputFile, InlineKeyboardMarkup, InlineKeyboardButton, CallbackQuery, ReplyKeyboardMarkup, KeyboardButton, BotCommand
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from aiogram.types.menu_button_commands import MenuButtonCommands
from dotenv import load_dotenv
from googletrans import Translator
from openai import OpenAI

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
BOT_TOKEN = getenv("BOT_TOKEN")
NVIDIA_API_KEY = getenv("NVIDIA_API_KEY")

# NVIDIA Whisper API
PARAKEET_API_URL = "https://ai.api.nvidia.com/v1/audio/transcription"

# OpenAI –∫–ª–∏–µ–Ω—Ç –¥–ª—è NVIDIA LLM
try:
    llm_client = OpenAI(
        base_url="https://integrate.api.nvidia.com/v1",
        api_key=NVIDIA_API_KEY,
        http_client=None
    )
except TypeError:
    # –î–ª—è Python 3.14+ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥—Ä—É–≥–æ–π —Å–ø–æ—Å–æ–±
    import httpx
    llm_client = OpenAI(
        base_url="https://integrate.api.nvidia.com/v1",
        api_key=NVIDIA_API_KEY,
        http_client=httpx.Client()
    )

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ –∏ —Ä–æ—É—Ç–µ—Ä–∞
bot = Bot(token=BOT_TOKEN)
router = Router()
translator = Translator()

# –•—Ä–∞–Ω–∏–ª–∏—â–µ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
user_models = {}

# –§–∞–π–ª –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
USER_DATA_FILE = Path("user_data.json")

# –ü–∞–∫–µ—Ç—ã –ø—Ä–µ–º–∏—É–º–∞
PREMIUM_TIERS = {
    "basic": {
        "name": "üü¶ Basic",
        "price": "199 —Ä—É–±",
        "limits": {
            "text": 30,
            "gemini": 25,
            "deepseek": 20,
            "claude": 15,
            "claude_sonnet": 18,
            "claude_haiku": 25,
            "claude_opus": 10,
            "qwen": 22,
            "llama": 28,
            "schnell": 5,
            "dev": 2,
            "kontext": 1
        }
    },
    "pro": {
        "name": "üü© Pro",
        "price": "499 —Ä—É–±",
        "limits": {
            "text": 100,
            "gemini": 80,
            "deepseek": 60,
            "claude": 50,
            "claude_sonnet": 60,
            "claude_haiku": 80,
            "claude_opus": 40,
            "qwen": 70,
            "llama": 90,
            "schnell": 15,
            "dev": 8,
            "kontext": 5
        }
    },
    "ultra": {
        "name": "üü• Ultra",
        "price": "999 —Ä—É–±",
        "limits": {
            "text": 250,
            "gemini": 200,
            "deepseek": 150,
            "claude": 120,
            "claude_sonnet": 150,
            "claude_haiku": 200,
            "claude_opus": 100,
            "qwen": 180,
            "llama": 220,
            "schnell": 40,
            "dev": 20,
            "kontext": 15
        }
    }
}


def load_user_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ —Ñ–∞–π–ª–∞"""
    if USER_DATA_FILE.exists():
        with open(USER_DATA_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"users": {}, "promocodes": {}}


def save_user_data(data):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ —Ñ–∞–π–ª"""
    with open(USER_DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


def get_user_limits(user_id: int):
    """–ü–æ–ª—É—á–∞–µ—Ç –ª–∏–º–∏—Ç—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    data = load_user_data()
    user_id_str = str(user_id)
    
    if user_id_str not in data["users"]:
        # –ù–æ–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å - –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –¥–æ—Å—Ç—É–ø
        data["users"][user_id_str] = {
            "tier": "free",
            "limits": {
                "text": 5,
                "gemini": 3,
                "deepseek": 2,
                "claude": 2,
                "claude_sonnet": 2,
                "claude_haiku": 3,
                "claude_opus": 1,
                "qwen": 3,
                "llama": 4,
                "schnell": 2,
                "dev": 0,
                "kontext": 0
            }
        }
        save_user_data(data)
    
    return data["users"][user_id_str]


def decrease_limit(user_id: int, model_key: str):
    """–£–º–µ–Ω—å—à–∞–µ—Ç –ª–∏–º–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    data = load_user_data()
    user_id_str = str(user_id)
    
    if user_id_str in data["users"]:
        if model_key in data["users"][user_id_str]["limits"]:
            data["users"][user_id_str]["limits"][model_key] -= 1
            save_user_data(data)
            return True
    return False


def check_limit(user_id: int, model_key: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ª–∏–º–∏—Ç"""
    user_data = get_user_limits(user_id)
    return user_data["limits"].get(model_key, 0) > 0


def get_user_history(user_id: int, model_key: str) -> list:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    data = load_user_data()
    user_id_str = str(user_id)
    
    if user_id_str in data["users"]:
        if "history" not in data["users"][user_id_str]:
            data["users"][user_id_str]["history"] = {}
            save_user_data(data)
        
        return data["users"][user_id_str]["history"].get(model_key, [])
    
    return []


def add_to_history(user_id: int, model_key: str, user_message: str, assistant_message: str):
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–º–∞–∫—Å–∏–º—É–º 20 —Å–æ–æ–±—â–µ–Ω–∏–π)"""
    data = load_user_data()
    user_id_str = str(user_id)
    
    if user_id_str in data["users"]:
        if "history" not in data["users"][user_id_str]:
            data["users"][user_id_str]["history"] = {}
        
        if model_key not in data["users"][user_id_str]["history"]:
            data["users"][user_id_str]["history"][model_key] = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        data["users"][user_id_str]["history"][model_key].append({
            "role": "user",
            "content": user_message
        })
        data["users"][user_id_str]["history"][model_key].append({
            "role": "assistant",
            "content": assistant_message
        })
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 20 —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ (10 –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç)
        if len(data["users"][user_id_str]["history"][model_key]) > 20:
            data["users"][user_id_str]["history"][model_key] = data["users"][user_id_str]["history"][model_key][-20:]
        
        save_user_data(data)


def clear_user_history(user_id: int, model_key: str = None):
    """–û—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
    data = load_user_data()
    user_id_str = str(user_id)
    
    if user_id_str in data["users"]:
        if model_key:
            # –û—á–∏—â–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
            if "history" in data["users"][user_id_str] and model_key in data["users"][user_id_str]["history"]:
                data["users"][user_id_str]["history"][model_key] = []
        else:
            # –û—á–∏—â–∞–µ–º –≤—Å—é –∏—Å—Ç–æ—Ä–∏—é
            data["users"][user_id_str]["history"] = {}
        
        save_user_data(data)


# –ú–æ–¥–µ–ª–∏ NVIDIA
MODELS = {
    "text": {
        "provider": "OpenAI",
        "name": "Chat GPT 5",
        "description": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
        "system_prompt": "You are ChatGPT-5, an advanced AI assistant created by OpenAI. You are a highly intelligent, helpful, and harmless AI that provides accurate, thoughtful, and comprehensive responses to user queries. You have extensive knowledge across all domains and can engage in complex reasoning, creative tasks, and problem-solving. Always respond in the language the user uses."
    },
    "gemini": {
        "provider": "Google",
        "name": "Gemini 2.0",
        "description": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
        "system_prompt": "You are Gemini 2.0, an advanced AI assistant created by Google. You are known for your multimodal capabilities, reasoning skills, and ability to understand complex contexts. You provide thoughtful, accurate, and comprehensive responses. Always respond in the language the user uses."
    },
    "deepseek": {
        "provider": "DeepSeek",
        "name": "DeepSeek R1",
        "description": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
        "system_prompt": "You are DeepSeek R1, an advanced reasoning AI assistant created by DeepSeek. You excel at logical reasoning, problem-solving, and providing detailed explanations. You think step-by-step and provide thorough analysis. Always respond in the language the user uses."
    },
    "claude": {
        "provider": "Anthropic",
        "name": "Claude 3.5",
        "description": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
        "system_prompt": "You are Claude 3.5, an advanced AI assistant created by Anthropic. You are known for your thoughtful analysis, nuanced understanding, and ethical reasoning. You provide balanced, insightful responses while being honest about limitations. Always respond in the language the user uses."
    },
    "claude_sonnet": {
        "provider": "Anthropic",
        "name": "Claude Sonnet 4.5",
        "description": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
        "system_prompt": "You are Claude Sonnet 4.5, an advanced AI assistant created by Anthropic. You are optimized for speed and efficiency while maintaining high quality reasoning. You excel at creative tasks and provide nuanced, thoughtful responses. Always respond in the language the user uses."
    },
    "claude_haiku": {
        "provider": "Anthropic",
        "name": "Claude Haiku 4.5",
        "description": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
        "system_prompt": "You are Claude Haiku 4.5, a lightweight yet capable AI assistant created by Anthropic. You are designed for quick, efficient responses while maintaining quality. You are helpful, harmless, and honest. Always respond in the language the user uses."
    },
    "claude_opus": {
        "provider": "Anthropic",
        "name": "Claude Opus 4.6",
        "description": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
        "system_prompt": "You are Claude Opus 4.6, the most advanced AI assistant created by Anthropic. You excel at complex reasoning, deep analysis, and nuanced understanding. You provide comprehensive, thoughtful responses to even the most challenging questions. Always respond in the language the user uses."
    },
    "qwen": {
        "provider": "Alibaba",
        "name": "Qwen 2.5",
        "description": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
        "system_prompt": "You are Qwen 2.5, an advanced AI assistant created by Alibaba. You are multilingual and excel at understanding diverse contexts and providing practical solutions. You are helpful, harmless, and honest. Always respond in the language the user uses."
    },
    "llama": {
        "provider": "Meta",
        "name": "Llama 3.1",
        "description": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
        "system_prompt": "You are Llama 3.1, an advanced AI assistant created by Meta. You are open-source and designed to be helpful, harmless, and honest. You provide clear, direct responses and excel at following instructions. Always respond in the language the user uses."
    },
    "schnell": {
        "provider": "Gemini",
        "url": "https://ai.api.nvidia.com/v1/genai/black-forest-labs/flux.1-schnell",
        "name": "NanoBanana 1",
        "description": "–ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (4 —à–∞–≥–∞)",
        "params": {
            "width": 1024,
            "height": 1024,
            "seed": 0,
            "steps": 4
        }
    },
    "dev": {
        "provider": "Gemini",
        "url": "https://ai.api.nvidia.com/v1/genai/black-forest-labs/flux.1-dev",
        "name": "NanoBanana 2",
        "description": "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (50 —à–∞–≥–æ–≤)",
        "params": {
            "width": 1024,
            "height": 1024,
            "seed": 0,
            "steps": 50,
            "cfg_scale": 3.5,
            "mode": "base"
        }
    },
    "kontext": {
        "provider": "Gemini",
        "url": "https://ai.api.nvidia.com/v1/genai/black-forest-labs/flux.1-kontext-dev",
        "name": "NanoBanana Edit",
        "description": "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (—Ç—Ä–µ–±—É–µ—Ç —Ñ–æ—Ç–æ)",
        "params": {
            "aspect_ratio": "match_input_image",
            "steps": 30,
            "cfg_scale": 3.5,
            "seed": 0
        }
    }
}

# –°–æ—Å—Ç–æ—è–Ω–∏—è FSM
class GenerationStates(StatesGroup):
    waiting_for_prompt = State()
    waiting_for_image = State()
    waiting_for_context_prompt = State()
    waiting_for_search_query = State()


def enhance_prompt(prompt: str) -> str:
    """–£–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–º–ø—Ç, –¥–æ–±–∞–≤–ª—è—è –¥–µ—Ç–∞–ª–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
    enhanced = f"{prompt}, high quality, detailed, professional, sharp focus, 8k resolution"
    return enhanced


def get_main_menu() -> ReplyKeyboardMarkup:
    """–°–æ–∑–¥–∞–µ—Ç –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é —Å —Ü–≤–µ—Ç–Ω—ã–º–∏ –∫–Ω–æ–ø–∫–∞–º–∏"""
    keyboard = ReplyKeyboardMarkup(
        keyboard=[
            [KeyboardButton(text="ü§ñ –ß—Ç–æ —É–º–µ–µ—Ç –±–æ—Ç", style="primary")],
            [KeyboardButton(text="üìù –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å", style="success"), KeyboardButton(text="üé® –°–æ–∑–¥–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É", style="primary")],
            [KeyboardButton(text="üîç –ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ–∏—Å–∫", style="danger"), KeyboardButton(text="üöÄ –ü—Ä–µ–º–∏—É–º", style="danger")],
        ],
        resize_keyboard=True,
        one_time_keyboard=False
    )
    return keyboard



async def generate_text(prompt: str, model_key: str = "text", user_id: int = None) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ NVIDIA LLM —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π"""
    try:
        logger.info(f"–ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞: {prompt[:100]}")

        model = MODELS.get(model_key, MODELS["text"])
        system_prompt = model.get("system_prompt", "You are a helpful AI assistant.")

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π —Å –∏—Å—Ç–æ—Ä–∏–µ–π
        messages = [{"role": "system", "content": system_prompt}]

        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –µ—Å—Ç—å user_id
        if user_id:
            history = get_user_history(user_id, model_key)
            messages.extend(history)

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        messages.append({"role": "user", "content": prompt})

        completion = llm_client.chat.completions.create(
            model="minimaxai/minimax-m2.5",
            messages=messages,
            temperature=1,
            top_p=0.95,
            max_tokens=8192,
            stream=False
        )

        generated_text = completion.choices[0].message.content

        # –£–¥–∞–ª—è–µ–º —Ç–µ–≥–∏ <think>...</think>
        import re
        generated_text = re.sub(r'<think>.*?</think>', '', generated_text, flags=re.DOTALL).strip()

        # –£–¥–∞–ª—è–µ–º markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (**, ##, ||, –∏ —Ç.–¥.)
        generated_text = re.sub(r'\*\*', '', generated_text)  # –£–¥–∞–ª—è–µ–º **
        generated_text = re.sub(r'##+ ', '', generated_text)  # –£–¥–∞–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        generated_text = re.sub(r'\|\s*-+\s*\|', '', generated_text)  # –£–¥–∞–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç–∞–±–ª–∏—Ü
        generated_text = re.sub(r'^\s*\|\s*$', '', generated_text, flags=re.MULTILINE)  # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å |

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –µ—Å—Ç—å user_id
        if user_id:
            add_to_history(user_id, model_key, prompt, generated_text)

        logger.info(f"–¢–µ–∫—Å—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {generated_text[:100]}")
        return generated_text

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
        raise Exception(f"–û—à–∏–±–∫–∞ LLM: {str(e)}")




async def translate_to_english(text: str) -> str:
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫"""
    try:
        result = await translator.translate(text, src_lang='auto', dest_lang='en')
        translated = result['text']
        logger.info(f"–ü–µ—Ä–µ–≤–æ–¥: '{text}' -> '{translated}'")
        return translated
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç.")
        return text


async def web_search(query: str) -> str:
    """–ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –±–µ–∑ API –∫–ª—é—á–µ–π —á–µ—Ä–µ–∑ DuckDuckGo"""
    try:
        logger.info(f"–ò—â—É: {query}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º DuckDuckGo —á–µ—Ä–µ–∑ httpx
        async with httpx.AsyncClient() as client:
            response = await client.get(
                "https://html.duckduckgo.com/html",
                params={"q": query},
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                },
                timeout=10
            )
            
            if response.status_code != 200:
                raise Exception(f"DuckDuckGo –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status_code}")
            
            # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            import re
            results = []
            
            # –ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            pattern = r'<a rel="nofollow" class="result__a" href="([^"]+)">([^<]+)</a>'
            matches = re.findall(pattern, response.text)
            
            for url, title in matches[:5]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                results.append(f"üîó {title}\n{url}")
            
            if results:
                return "\n\n".join(results)
            else:
                return "‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
                
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
        return f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}"


def _upload_asset(input_data: bytes, description: str) -> str:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ NVCF API –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç asset ID"""
    try:
        header_auth = f"Bearer {NVIDIA_API_KEY}"
        
        authorize = requests.post(
            "https://api.nvcf.nvidia.com/v2/nvcf/assets",
            headers={
                "Authorization": header_auth,
                "Content-Type": "application/json",
                "accept": "application/json",
            },
            json={"contentType": "image/jpeg", "description": description},
            timeout=30,
        )
        authorize.raise_for_status()
        
        upload_url = authorize.json()["uploadUrl"]
        asset_id = authorize.json()["assetId"]
        
        response = requests.put(
            upload_url,
            data=input_data,
            headers={
                "x-amz-meta-nvcf-asset-description": description,
                "content-type": "image/jpeg",
            },
            timeout=300,
        )
        response.raise_for_status()
        
        logger.info(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å ID: {asset_id}")
        return asset_id
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        raise Exception(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}")


async def compare_images_changenet(reference_image: bytes, test_image: bytes) -> bytes:
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é Visual ChangeNet"""
    try:
        logger.info("–ó–∞–≥—Ä—É–∂–∞—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        asset_id1 = _upload_asset(reference_image, "Reference Image")
        asset_id2 = _upload_asset(test_image, "Test Image")
        
        logger.info(f"Asset IDs: {asset_id1}, {asset_id2}")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
        nvai_url = "https://ai.api.nvidia.com/v1/cv/nvidia/visual-changenet"
        header_auth = f"Bearer {NVIDIA_API_KEY}"
        
        inputs = {
            "reference_image": str(asset_id1),
            "test_image": str(asset_id2)
        }
        
        asset_list = f"{asset_id1},{asset_id2}"
        
        headers = {
            "Content-Type": "application/json",
            "NVCF-INPUT-ASSET-REFERENCES": asset_list,
            "NVCF-FUNCTION-ASSET-IDS": asset_list,
            "Authorization": header_auth,
        }
        
        logger.info("–û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –∫ Visual ChangeNet...")
        
        response = requests.post(nvai_url, headers=headers, json=inputs, timeout=300)
        response.raise_for_status()
        
        logger.info("–û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω, —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º zip –≤ –ø–∞–º—è—Ç–∏
        zip_buffer = BytesIO(response.content)
        
        # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∏ –∏—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        with zipfile.ZipFile(zip_buffer, 'r') as z:
            files = z.namelist()
            logger.info(f"–§–∞–π–ª—ã –≤ –∞—Ä—Ö–∏–≤–µ: {files}")
            
            # –ò—â–µ–º —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–æ–±—ã—á–Ω–æ —ç—Ç–æ PNG –∏–ª–∏ JPG)
            result_file = None
            for f in files:
                if f.endswith(('.png', '.jpg', '.jpeg')):
                    result_file = f
                    break
            
            if result_file:
                result_bytes = z.read(result_file)
                logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–ª—É—á–µ–Ω: {result_file}")
                return result_bytes
            else:
                raise Exception("–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∞—Ä—Ö–∏–≤–µ")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
        raise Exception(f"–û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {str(e)}")


async def generate_image(prompt: str, model_key: str, image_data: str = None) -> tuple[bytes, dict]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ NVIDIA API"""
    model = MODELS.get(model_key, MODELS["schnell"])
    
    enhanced_prompt = enhance_prompt(prompt)
    
    logger.info(f"–ú–æ–¥–µ–ª—å: {model['name']}")
    logger.info(f"–ò—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç: {prompt}")
    logger.info(f"–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç: {enhanced_prompt}")
    
    headers = {
        "Authorization": f"Bearer {NVIDIA_API_KEY}",
        "Accept": "application/json",
    }
    
    payload = {
        "prompt": enhanced_prompt,
        **model["params"]
    }
    
    if model_key == "kontext" and image_data:
        payload["image"] = image_data
    
    logger.info(f"–û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –∫ {model['url']}")
    
    async with aiohttp.ClientSession() as session:
        try:
            async with session.post(
                model["url"],
                json=payload,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=180)
            ) as response:
                response_text = await response.text()
                logger.info(f"–°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞: {response.status}")
                logger.info(f"–û—Ç–≤–µ—Ç API: {response_text[:500]}")
                
                request_info = {
                    "request_id": response.headers.get("Nvcf-Reqid", "N/A"),
                    "status": response.headers.get("Nvcf-Status", "N/A"),
                    "model": model["name"],
                }
                
                logger.info(f"Request ID: {request_info['request_id']}, Status: {request_info['status']}")
                
                if response.status != 200:
                    raise Exception(f"API –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É: {response.status}\n{response_text[:200]}")
                
                result = await response.json()
                
                if "artifacts" in result and len(result["artifacts"]) > 0:
                    image_b64 = result["artifacts"][0].get("base64", "")
                elif "image" in result:
                    image_b64 = result["image"]
                elif "data" in result and len(result["data"]) > 0:
                    image_b64 = result["data"][0].get("b64_json", "")
                else:
                    raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –æ—Ç–≤–µ—Ç–µ API")
                
                image_bytes = base64.b64decode(image_b64)
                return image_bytes, request_info
                
        except asyncio.TimeoutError:
            logger.error("Timeout –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ NVIDIA API")
            raise Exception("–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç API")
        except aiohttp.ClientError as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}")
            raise Exception("–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å API")


@router.message(CommandStart())
async def cmd_start(message: Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
    await message.answer(
        "üëã –ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é NVIDIA.\n\n"
        "–í—ã–±–µ—Ä–∏, —á—Ç–æ —Ç–µ–±–µ –Ω—É–∂–Ω–æ:",
        reply_markup=get_main_menu()
    )


@router.message(Command("ask"))
async def cmd_ask(message: Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /ask - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    if not message.text or message.text == "/ask":
        await message.answer("–ò—Å–ø–æ–ª—å–∑—É–π: /ask <—Ç–≤–æ–π –≤–æ–ø—Ä–æ—Å>\n\n–ù–∞–ø—Ä–∏–º–µ—Ä: /ask –ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?")
        return
    
    prompt = message.text.replace("/ask ", "", 1)
    user_id = message.from_user.id
    model_key = user_models.get(user_id, "text")
    status_msg = await message.answer("ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...")
    
    try:
        response_text = await generate_text(prompt, model_key, user_id)
        
        if len(response_text) > 4096:
            for i in range(0, len(response_text), 4096):
                await message.answer(response_text[i:i+4096])
        else:
            await message.answer(response_text)
        
        await status_msg.delete()
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
        await status_msg.edit_text(
            f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞:\n{str(e)}\n\n"
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑."
        )


@router.message(F.text == "ü§ñ –ß—Ç–æ —É–º–µ–µ—Ç –±–æ—Ç")
async def btn_about_bot(message: Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–Ω–æ–ø–∫–∏ '–ß—Ç–æ —É–º–µ–µ—Ç –±–æ—Ç'"""
    await message.answer(
        "ü§ñ –Ø —É–º–µ—é:\n\n"
        "üìù –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç - –æ—Ç–≤–µ—á—É –Ω–∞ –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å\n"
        "üé® –°–æ–∑–¥–∞–≤–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∏ - –ø–æ —Ç–≤–æ–µ–º—É –æ–ø–∏—Å–∞–Ω–∏—é\n"
        "üîç –ò—Å–∫–∞—Ç—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ - –Ω–∞–π–¥—É –Ω—É–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n\n"
        "–í—ã–±–µ—Ä–∏, —á—Ç–æ —Ç–µ–±–µ –Ω—É–∂–Ω–æ!",
        reply_markup=get_main_menu()
    )


@router.message(F.text == "üìù –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å")
async def btn_select_model(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–Ω–æ–ø–∫–∏ '–í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å'"""
    await cmd_model(message, state)


@router.message(F.text == "üé® –°–æ–∑–¥–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É")
async def btn_create_image(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–Ω–æ–ø–∫–∏ '–°–æ–∑–¥–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É'"""
    keyboard = InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text="‚ö° –ë—ã—Å—Ç—Ä–∞—è (4 —à–∞–≥–∞)", callback_data="model_schnell", style="success")],
        [InlineKeyboardButton(text="üé® –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è (50 —à–∞–≥–æ–≤)", callback_data="model_dev", style="success")],
        [InlineKeyboardButton(text="üñºÔ∏è –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ç–æ", callback_data="model_kontext", style="success")],
    ])
    await message.answer("üé® –í—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏:", reply_markup=keyboard)


@router.message(F.text == "üîç –ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ–∏—Å–∫")
async def btn_web_search(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–Ω–æ–ø–∫–∏ '–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ–∏—Å–∫'"""
    await message.answer("üîç –í–≤–µ–¥–∏, —á—Ç–æ —Ç—ã —Ö–æ—á–µ—à—å –Ω–∞–π—Ç–∏:")
    await state.set_state(GenerationStates.waiting_for_search_query)


@router.message(F.text == "üöÄ –ü—Ä–µ–º–∏—É–º")
async def btn_premium(message: Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–Ω–æ–ø–∫–∏ '–ü—Ä–µ–º–∏—É–º'"""
    user_id = message.from_user.id
    user_data = get_user_limits(user_id)

    keyboard = InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text="üü¶ Basic - 199 —Ä—É–±", callback_data="premium_basic", style="primary")],
        [InlineKeyboardButton(text="üü© Pro - 499 —Ä—É–±", callback_data="premium_pro", style="success")],
        [InlineKeyboardButton(text="üü• Ultra - 999 —Ä—É–±", callback_data="premium_ultra", style="danger")],
        [InlineKeyboardButton(text="üí¨ –°–≤—è–∑–∞—Ç—å—Å—è —Å –∞–¥–º–∏–Ω–æ–º", url="https://t.me/korzina_dar")],
    ])

    current_tier = user_data.get("tier", "free")
    tier_name = PREMIUM_TIERS.get(current_tier, {}).get("name", "–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π")

    await message.answer(
        f"üöÄ –ü—Ä–µ–º–∏—É–º –ø–æ–¥–ø–∏—Å–∫–∞\n\n"
        f"–¢–µ–∫—É—â–∏–π –ø–∞–∫–µ—Ç: {tier_name}\n\n"
        f"–í—ã–±–µ—Ä–∏ –ø–∞–∫–µ—Ç –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–µ—Ç–∞–ª–µ–π:",
        reply_markup=keyboard
    )


@router.callback_query(F.data.startswith("premium_"))
async def show_premium_tier(query: CallbackQuery):
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –ø—Ä–µ–º–∏—É–º –ø–∞–∫–µ—Ç–∞"""
    tier = query.data.split("_")[1]
    tier_data = PREMIUM_TIERS[tier]
    
    limits_text = "\n".join([
        f"‚Ä¢ Chat GPT 5: {tier_data['limits']['text']} –∑–∞–ø—Ä–æ—Å–æ–≤",
        f"‚Ä¢ Gemini 2.0: {tier_data['limits']['gemini']} –∑–∞–ø—Ä–æ—Å–æ–≤",
        f"‚Ä¢ DeepSeek R1: {tier_data['limits']['deepseek']} –∑–∞–ø—Ä–æ—Å–æ–≤",
        f"‚Ä¢ Claude 3.5: {tier_data['limits']['claude']} –∑–∞–ø—Ä–æ—Å–æ–≤",
        f"‚Ä¢ Qwen 2.5: {tier_data['limits']['qwen']} –∑–∞–ø—Ä–æ—Å–æ–≤",
        f"‚Ä¢ Llama 3.1: {tier_data['limits']['llama']} –∑–∞–ø—Ä–æ—Å–æ–≤",
        f"‚Ä¢ –ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {tier_data['limits']['schnell']} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
        f"‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {tier_data['limits']['dev']} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
        f"‚Ä¢ –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ç–æ: {tier_data['limits']['kontext']} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
    ])
    
    await query.message.edit_text(
        f"{tier_data['name']}\n"
        f"–°—Ç–æ–∏–º–æ—Å—Ç—å: {tier_data['price']}\n\n"
        f"–õ–∏–º–∏—Ç—ã:\n{limits_text}\n\n"
        f"–î–ª—è –ø–æ–∫—É–ø–∫–∏ –ø—Ä–æ–º–æ–∫–æ–¥–∞ —Å–≤—è–∂–∏—Ç–µ—Å—å —Å –∞–¥–º–∏–Ω–æ–º: @korzina_dar\n"
        f"–ü–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–º–æ–∫–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: /promo –ö–û–î",
        reply_markup=InlineKeyboardMarkup(inline_keyboard=[
            [InlineKeyboardButton(text="üí¨ –°–≤—è–∑–∞—Ç—å—Å—è —Å –∞–¥–º–∏–Ω–æ–º", url="https://t.me/korzina_dar")],
            [InlineKeyboardButton(text="‚óÄÔ∏è –ù–∞–∑–∞–¥", callback_data="premium_back")]
        ])
    )
    await query.answer()


@router.callback_query(F.data == "premium_back")
async def premium_back(query: CallbackQuery):
    """–í–æ–∑–≤—Ä–∞—Ç –∫ –≤—ã–±–æ—Ä—É –ø–∞–∫–µ—Ç–æ–≤"""
    user_id = query.from_user.id
    user_data = get_user_limits(user_id)
    
    keyboard = InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text="üü¶ Basic - 199 —Ä—É–±", callback_data="premium_basic", style="primary")],
        [InlineKeyboardButton(text="üü© Pro - 499 —Ä—É–±", callback_data="premium_pro", style="success")],
        [InlineKeyboardButton(text="üü• Ultra - 999 —Ä—É–±", callback_data="premium_ultra", style="danger")],
        [InlineKeyboardButton(text="üí¨ –°–≤—è–∑–∞—Ç—å—Å—è —Å –∞–¥–º–∏–Ω–æ–º", url="https://t.me/korzina_dar")],
    ])
    
    current_tier = user_data.get("tier", "free")
    tier_name = PREMIUM_TIERS.get(current_tier, {}).get("name", "–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π")
    
    await query.message.edit_text(
        f"üöÄ –ü—Ä–µ–º–∏—É–º –ø–æ–¥–ø–∏—Å–∫–∞\n\n"
        f"–¢–µ–∫—É—â–∏–π –ø–∞–∫–µ—Ç: {tier_name}\n\n"
        f"–í—ã–±–µ—Ä–∏ –ø–∞–∫–µ—Ç –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–µ—Ç–∞–ª–µ–π:",
        reply_markup=keyboard
    )
    await query.answer()


@router.message(Command("promo"))
async def cmd_promo(message: Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /promo - –∞–∫—Ç–∏–≤–∞—Ü–∏—è –ø—Ä–æ–º–æ–∫–æ–¥–∞"""
    if not message.text or len(message.text.split()) < 2:
        await message.answer(
            "‚ùå –ò—Å–ø–æ–ª—å–∑—É–π: /promo –ö–û–î\n\n"
            "–ù–∞–ø—Ä–∏–º–µ—Ä: /promo BASIC10A\n\n"
            "–ü—Ä–æ–º–æ–∫–æ–¥—ã –º–æ–∂–Ω–æ –∫—É–ø–∏—Ç—å —É –∞–¥–º–∏–Ω–∞: @korzina_dar"
        )
        return
    
    promo_code = message.text.split()[1].upper()
    user_id = message.from_user.id
    
    data = load_user_data()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–æ–∫–æ–¥–∞
    if promo_code not in data["promocodes"]:
        await message.answer("‚ùå –ü—Ä–æ–º–æ–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –ª–∏ –ø—Ä–æ–º–æ–∫–æ–¥
    if data["promocodes"][promo_code]["used"]:
        await message.answer("‚ùå –≠—Ç–æ—Ç –ø—Ä–æ–º–æ–∫–æ–¥ —É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω!")
        return
    
    # –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º –ø—Ä–æ–º–æ–∫–æ–¥
    tier = data["promocodes"][promo_code]["tier"]
    tier_data = PREMIUM_TIERS[tier]
    
    user_id_str = str(user_id)
    data["users"][user_id_str] = {
        "tier": tier,
        "limits": tier_data["limits"].copy()
    }
    data["promocodes"][promo_code]["used"] = True
    data["promocodes"][promo_code]["used_by"] = user_id
    
    save_user_data(data)
    
    await message.answer(
        f"‚úÖ –ü—Ä–æ–º–æ–∫–æ–¥ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω!\n\n"
        f"–ü–∞–∫–µ—Ç: {tier_data['name']}\n\n"
        f"–¢–≤–æ–∏ –ª–∏–º–∏—Ç—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–π /limits —á—Ç–æ–±—ã –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –æ—Å—Ç–∞—Ç–∫–∏."
    )


@router.message(Command("limits"))
async def cmd_limits(message: Message):
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ª–∏–º–∏—Ç—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    user_id = message.from_user.id
    user_data = get_user_limits(user_id)
    
    tier_name = PREMIUM_TIERS.get(user_data["tier"], {}).get("name", "–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π")
    limits = user_data["limits"]
    
    limits_text = "\n".join([
        f"‚Ä¢ Chat GPT 5: {limits.get('text', 0)} –∑–∞–ø—Ä–æ—Å–æ–≤",
        f"‚Ä¢ Gemini 2.0: {limits.get('gemini', 0)} –∑–∞–ø—Ä–æ—Å–æ–≤",
        f"‚Ä¢ DeepSeek R1: {limits.get('deepseek', 0)} –∑–∞–ø—Ä–æ—Å–æ–≤",
        f"‚Ä¢ Claude 3.5: {limits.get('claude', 0)} –∑–∞–ø—Ä–æ—Å–æ–≤",
        f"‚Ä¢ Qwen 2.5: {limits.get('qwen', 0)} –∑–∞–ø—Ä–æ—Å–æ–≤",
        f"‚Ä¢ Llama 3.1: {limits.get('llama', 0)} –∑–∞–ø—Ä–æ—Å–æ–≤",
        f"‚Ä¢ –ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {limits.get('schnell', 0)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
        f"‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {limits.get('dev', 0)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
        f"‚Ä¢ –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ç–æ: {limits.get('kontext', 0)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
    ])
    
    await message.answer(
        f"üìä –¢–≤–æ–∏ –ª–∏–º–∏—Ç—ã\n\n"
        f"–ü–∞–∫–µ—Ç: {tier_name}\n\n"
        f"{limits_text}\n\n"
        f"–î–ª—è –ø–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π /promo –∏–ª–∏ –∫—É–ø–∏ –Ω–æ–≤—ã–π –ø–∞–∫–µ—Ç —á–µ—Ä–µ–∑ üöÄ –ü—Ä–µ–º–∏—É–º"
    )


@router.message(Command("model"))
async def cmd_model(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /model - –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏"""
    keyboard = InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text="üìù Chat GPT 5", callback_data="model_text", style="primary")],
        [InlineKeyboardButton(text="üîµ Gemini 2.0", callback_data="model_gemini", style="primary")],
        [InlineKeyboardButton(text="üü£ DeepSeek R1", callback_data="model_deepseek", style="primary")],
        [InlineKeyboardButton(text="üü† Claude 3.5", callback_data="model_claude", style="primary")],
        [InlineKeyboardButton(text="üü† Claude Sonnet 4.5", callback_data="model_claude_sonnet", style="primary")],
        [InlineKeyboardButton(text="üü† Claude Haiku 4.5", callback_data="model_claude_haiku", style="primary")],
        [InlineKeyboardButton(text="üü† Claude Opus 4.6", callback_data="model_claude_opus", style="primary")],
        [InlineKeyboardButton(text="üü° Qwen 2.5", callback_data="model_qwen", style="primary")],
        [InlineKeyboardButton(text="üî¥ Llama 3.1", callback_data="model_llama", style="primary")],
        [InlineKeyboardButton(text="‚ö° –ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è", callback_data="model_schnell", style="success")],
        [InlineKeyboardButton(text="üé® –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è", callback_data="model_dev", style="success")],
        [InlineKeyboardButton(text="üñºÔ∏è –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ç–æ", callback_data="model_kontext", style="success")],
    ])
    
    await message.answer("üéØ –í—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å:", reply_markup=keyboard)


@router.callback_query(F.data.startswith("model_"))
async def select_model(query: CallbackQuery, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏"""
    model_key = query.data.split("_", 1)[1]
    user_models[query.from_user.id] = model_key
    model = MODELS[model_key]
    
    await query.answer()
    
    # –¢–µ–∫—Å—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
    if model_key in ["text", "gemini", "deepseek", "claude", "claude_sonnet", "claude_haiku", "claude_opus", "qwen", "llama"]:
        await query.message.edit_text(
            f"‚úÖ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {model['name']}\n\n"
            f"–¢–µ–ø–µ—Ä—å –æ—Ç–ø—Ä–∞–≤—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ —Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."
        )
        await state.set_state(GenerationStates.waiting_for_prompt)
    elif model_key == "kontext":
        await query.message.edit_text(
            f"‚úÖ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {model['name']}\n\n"
            f"–û—Ç–ø—Ä–∞–≤—å —Ñ–æ—Ç–æ, –∫–æ—Ç–æ—Ä–æ–µ –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å, –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π."
        )
        await state.set_state(GenerationStates.waiting_for_image)
    else:
        # –ú–æ–¥–µ–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        await query.message.edit_text(
            f"‚úÖ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {model['name']}\n\n"
            f"–¢–µ–ø–µ—Ä—å –æ—Ç–ø—Ä–∞–≤—å —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."
        )
        await state.set_state(GenerationStates.waiting_for_prompt)


@router.message(F.photo)
async def handle_photo(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–æ—Ç–æ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    user_id = message.from_user.id
    
    if user_models.get(user_id) != "kontext":
        await message.answer("–°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å /model (FLUX.1-kontext-dev –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–æ—Ç–æ)")
        return
    
    photo = message.photo[-1]
    file = await bot.get_file(photo.file_id)
    file_bytes = await bot.download_file(file.file_path)
    
    image_b64 = base64.b64encode(file_bytes.getvalue()).decode()
    image_data = f"data:image/jpeg;base64,{image_b64}"
    
    await state.update_data(image_data=image_data)
    
    await message.answer("üì∏ –§–æ—Ç–æ –ø–æ–ª—É—á–µ–Ω–æ! –¢–µ–ø–µ—Ä—å –æ—Ç–ø—Ä–∞–≤—å –æ–ø–∏—Å–∞–Ω–∏–µ, —á—Ç–æ –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å.")
    await state.set_state(GenerationStates.waiting_for_context_prompt)


@router.message(GenerationStates.waiting_for_context_prompt)
async def handle_context_prompt(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    prompt = message.text
    user_id = message.from_user.id
    model_key = user_models.get(user_id, "schnell")
    
    status_msg = await message.answer("üé® –ì–µ–Ω–µ—Ä–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –ø–æ–¥–æ–∂–¥–∏—Ç–µ...")
    
    try:
        data = await state.get_data()
        image_data = data.get("image_data")
        
        image_bytes, request_info = await generate_image(prompt, model_key, image_data)
        
        image_file = BufferedInputFile(file=image_bytes, filename="generated_image.png")
        
        caption = f"‚ú® –ì–æ—Ç–æ–≤–æ!\n\n–ú–æ–¥–µ–ª—å: {request_info['model']}\n–ü—Ä–æ–º–ø—Ç: {prompt}"
        if request_info["request_id"] != "N/A":
            caption += f"\n\nüîë ID –∑–∞–ø—Ä–æ—Å–∞: {request_info['request_id']}"
        
        await message.answer_photo(photo=image_file, caption=caption)
        await status_msg.delete()
        await state.clear()
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        await status_msg.edit_text(
            f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:\n{str(e)}\n\n"
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ."
        )
        await state.clear()


@router.message(GenerationStates.waiting_for_search_query)
async def handle_search_query(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    query = message.text
    status_msg = await message.answer("üîç –ò—â—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")
    
    try:
        results = await web_search(query)
        
        if len(results) > 4096:
            for i in range(0, len(results), 4096):
                await message.answer(results[i:i+4096])
        else:
            await message.answer(results)
        
        await status_msg.delete()
        await state.clear()
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
        await status_msg.edit_text(
            f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ:\n{str(e)}\n\n"
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑."
        )
        await state.clear()


@router.message(GenerationStates.waiting_for_prompt)
async def handle_prompt(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø—Ä–æ–º–ø—Ç–∞"""
    prompt = message.text
    user_id = message.from_user.id
    model_key = user_models.get(user_id, "text")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
    if not check_limit(user_id, model_key):
        await message.answer(
            f"‚ùå –£ —Ç–µ–±—è –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏!\n\n"
            f"–ò—Å–ø–æ–ª—å–∑—É–π /limits —á—Ç–æ–±—ã –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –æ—Å—Ç–∞—Ç–∫–∏ –∏–ª–∏ –∫—É–ø–∏ –Ω–æ–≤—ã–π –ø–∞–∫–µ—Ç —á–µ—Ä–µ–∑ üöÄ –ü—Ä–µ–º–∏—É–º"
        )
        await state.clear()
        return
    
    model = MODELS.get(model_key, MODELS["text"])
    
    # –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤–∞—è –º–æ–¥–µ–ª—å
    if model_key in ["text", "gemini", "deepseek", "claude", "claude_sonnet", "claude_haiku", "claude_opus", "qwen", "llama"]:
        status_msg = await message.answer("ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...")
        try:
            response_text = await generate_text(prompt, model_key, user_id)
            
            # –£–º–µ–Ω—å—à–∞–µ–º –ª–∏–º–∏—Ç –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            decrease_limit(user_id, model_key)
            
            if len(response_text) > 4096:
                for i in range(0, len(response_text), 4096):
                    await message.answer(response_text[i:i+4096])
            else:
                await message.answer(response_text)
            
            await status_msg.delete()
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Å—Ç–∞—Ç–æ–∫
            user_data = get_user_limits(user_id)
            remaining = user_data["limits"].get(model_key, 0)
            await message.answer(f"üìä –û—Å—Ç–∞–ª–æ—Å—å –∑–∞–ø—Ä–æ—Å–æ–≤: {remaining}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            await status_msg.edit_text(
                f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞:\n{str(e)}\n\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑."
            )
    else:
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        status_msg = await message.answer("üé® –ì–µ–Ω–µ—Ä–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –ø–æ–¥–æ–∂–¥–∏—Ç–µ...")
        
        try:
            image_bytes, request_info = await generate_image(prompt, model_key)
            
            # –£–º–µ–Ω—å—à–∞–µ–º –ª–∏–º–∏—Ç –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            decrease_limit(user_id, model_key)
            
            image_file = BufferedInputFile(file=image_bytes, filename="generated_image.png")
            
            user_data = get_user_limits(user_id)
            remaining = user_data["limits"].get(model_key, 0)
            
            caption = f"‚ú® –ì–æ—Ç–æ–≤–æ!\n\n–ú–æ–¥–µ–ª—å: {request_info['model']}\n–ü—Ä–æ–º–ø—Ç: {prompt}\n\nüìä –û—Å—Ç–∞–ª–æ—Å—å: {remaining}"
            if request_info["request_id"] != "N/A":
                caption += f"\nüîë ID: {request_info['request_id']}"
            
            await message.answer_photo(photo=image_file, caption=caption)
            await status_msg.delete()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            await status_msg.edit_text(
                f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:\n{str(e)}\n\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ."
            )
    
    await state.clear()


@router.message(F.text.startswith("/"))
async def handle_unknown_command(message: Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–æ–º–∞–Ω–¥"""
    command = message.text.split()[0]
    
    commands_list = """‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: {}\n\nüìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n\n/start - –ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é\n/ask - –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å\n/model - –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å\n/help - –°–ø—Ä–∞–≤–∫–∞""".format(command)
    
    await message.answer(commands_list, reply_markup=get_main_menu())


@router.message(F.text)
async def handle_text(message: Message, state: FSMContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    prompt = message.text
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω–∞—Ö–æ–¥–∏–º—Å—è –ª–∏ –º—ã –≤ –∫–∞–∫–æ–º-—Ç–æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏
    current_state = await state.get_state()
    if current_state is not None:
        # –ï—Å–ª–∏ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º - –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
        return
    
    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã –∫–Ω–æ–ø–æ–∫ –º–µ–Ω—é
    menu_buttons = [
        "ü§ñ –ß—Ç–æ —É–º–µ–µ—Ç –±–æ—Ç",
        "üìù –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å",
        "üé® –°–æ–∑–¥–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É",
        "üîç –ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ–∏—Å–∫",
        "üöÄ –ü—Ä–µ–º–∏—É–º"
    ]
    
    if prompt in menu_buttons:
        return
    
    user_id = message.from_user.id
    model_key = user_models.get(user_id, "text")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
    if not check_limit(user_id, model_key):
        await message.answer(
            f"‚ùå –£ —Ç–µ–±—è –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏!\n\n"
            f"–ò—Å–ø–æ–ª—å–∑—É–π /limits —á—Ç–æ–±—ã –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –æ—Å—Ç–∞—Ç–∫–∏ –∏–ª–∏ –∫—É–ø–∏ –Ω–æ–≤—ã–π –ø–∞–∫–µ—Ç —á–µ—Ä–µ–∑ üöÄ –ü—Ä–µ–º–∏—É–º"
        )
        return
    
    model = MODELS.get(model_key, MODELS["text"])
    
    # –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤–∞—è –º–æ–¥–µ–ª—å
    if model_key in ["text", "gemini", "deepseek", "claude", "claude_sonnet", "claude_haiku", "claude_opus", "qwen", "llama"]:
        status_msg = await message.answer("ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...")
        try:
            response_text = await generate_text(prompt, model_key, user_id)
            
            # –£–º–µ–Ω—å—à–∞–µ–º –ª–∏–º–∏—Ç –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            decrease_limit(user_id, model_key)
            
            if len(response_text) > 4096:
                for i in range(0, len(response_text), 4096):
                    await message.answer(response_text[i:i+4096])
            else:
                await message.answer(response_text)
            
            await status_msg.delete()
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Å—Ç–∞—Ç–æ–∫
            user_data = get_user_limits(user_id)
            remaining = user_data["limits"].get(model_key, 0)
            await message.answer(f"üìä –û—Å—Ç–∞–ª–æ—Å—å –∑–∞–ø—Ä–æ—Å–æ–≤: {remaining}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            await status_msg.edit_text(
                f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞:\n{str(e)}\n\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑."
            )
    else:
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        status_msg = await message.answer("üé® –ì–µ–Ω–µ—Ä–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –ø–æ–¥–æ–∂–¥–∏—Ç–µ...")
        
        try:
            image_bytes, request_info = await generate_image(prompt, model_key)
            
            # –£–º–µ–Ω—å—à–∞–µ–º –ª–∏–º–∏—Ç –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            decrease_limit(user_id, model_key)
            
            image_file = BufferedInputFile(file=image_bytes, filename="generated_image.png")
            
            user_data = get_user_limits(user_id)
            remaining = user_data["limits"].get(model_key, 0)
            
            caption = f"‚ú® –ì–æ—Ç–æ–≤–æ!\n\n–ú–æ–¥–µ–ª—å: {request_info['model']}\n–ü—Ä–æ–º–ø—Ç: {prompt}\n\nüìä –û—Å—Ç–∞–ª–æ—Å—å: {remaining}"
            if request_info["request_id"] != "N/A":
                caption += f"\nüîë ID: {request_info['request_id']}"
            
            await message.answer_photo(photo=image_file, caption=caption)
            await status_msg.delete()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            await status_msg.edit_text(
                f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:\n{str(e)}\n\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ."
            )


async def setup_bot_commands(bot: Bot):
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–º–∞–Ω–¥ –¥–ª—è –±–æ—Ç–∞"""
    commands = [
        BotCommand(command="start", description="ü§ñ –ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é"),
        BotCommand(command="ask", description="üìù –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å"),
        BotCommand(command="model", description="üé® –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å"),
        BotCommand(command="promo", description="üéÅ –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–º–æ–∫–æ–¥"),
        BotCommand(command="limits", description="üìä –ú–æ–∏ –ª–∏–º–∏—Ç—ã"),
        BotCommand(command="help", description="‚ùì –°–ø—Ä–∞–≤–∫–∞"),
    ]
    
    await bot.set_my_commands(commands)
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–Ω–æ–ø–∫—É –º–µ–Ω—é
    menu_button = MenuButtonCommands()
    await bot.set_chat_menu_button(menu_button=menu_button)
    
    logger.info("–ö–æ–º–∞–Ω–¥—ã –±–æ—Ç–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")


@router.message(Command("help"))
async def cmd_help(message: Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /help"""
    await message.answer(
        "‚ùì –°–ø—Ä–∞–≤–∫–∞\n\n"
        "–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n\n"
        "/start - –û—Ç–∫—Ä—ã—Ç—å –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é\n"
        "/ask <–≤–æ–ø—Ä–æ—Å> - –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –±–æ—Ç—É\n"
        "/model - –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n"
        "/help - –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç—É —Å–ø—Ä–∞–≤–∫—É\n\n"
        "–ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π –∫–Ω–æ–ø–∫–∏ –º–µ–Ω—é –Ω–∏–∂–µ!",
        reply_markup=get_main_menu()
    )


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞"""
    dp = Dispatcher()
    dp.include_router(router)
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–º–∞–Ω–¥—ã –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
    await setup_bot_commands(bot)
    
    logger.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
    
    await dp.start_polling(bot)


if __name__ == "__main__":
    asyncio.run(main())


